{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import related modules/libraries/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\Ankit\\Desktop\\case_study_bcg\\solutions\n",
      "Project Root Directory: c:\\Users\\Ankit\\Desktop\\case_study_bcg\n"
     ]
    }
   ],
   "source": [
    "# All built in imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate to the project root directory (one level up from 'solutions')\n",
    "project_root = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the project root to sys.path for dynamic imports\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Current Working Directory:\", current_dir)\n",
    "print(\"Project Root Directory:\", project_root)\n",
    "\n",
    "\n",
    "# All custom imports\n",
    "from modules.setup import read_all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize spark and read all data from raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version is: 3.5.3\n",
      "\n",
      "------ Printing config below ------\n",
      "{'input_files': {'charges': {'file_type': 'csv', 'input_file_path': '../raw_data/Charges_use.csv'}, 'damages': {'file_type': 'csv', 'input_file_path': '../raw_data/Damages_use.csv'}, 'endorse_use': {'file_type': 'csv', 'input_file_path': '../raw_data/Endorse_use.csv'}, 'primary_person_use': {'file_type': 'csv', 'input_file_path': '../raw_data/Primary_Person_use.csv'}, 'restrict_use': {'file_type': 'csv', 'input_file_path': '../raw_data/Restrict_use.csv'}, 'units_use': {'file_type': 'csv', 'input_file_path': '../raw_data/Units_use.csv'}}}\n",
      "------ Config ends ------ \n",
      "\n",
      "Reading dataset: charges:\n",
      "+--------+--------+--------+--------------------+------------+\n",
      "|CRASH_ID|UNIT_NBR|PRSN_NBR|              CHARGE|CITATION_NBR|\n",
      "+--------+--------+--------+--------------------+------------+\n",
      "|14768622|       1|       1|DRIVING WHILE INT...|        NULL|\n",
      "+--------+--------+--------+--------------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "\n",
      "\n",
      "Reading dataset: damages:\n",
      "+--------+----------------+\n",
      "|CRASH_ID|DAMAGED_PROPERTY|\n",
      "+--------+----------------+\n",
      "|14768622|         MAILBOX|\n",
      "+--------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "\n",
      "\n",
      "Reading dataset: endorse_use:\n",
      "+--------+--------+------------------+\n",
      "|CRASH_ID|UNIT_NBR|DRVR_LIC_ENDORS_ID|\n",
      "+--------+--------+------------------+\n",
      "|14768622|       1|              NONE|\n",
      "+--------+--------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "\n",
      "\n",
      "Reading dataset: primary_person_use:\n",
      "+--------+--------+--------+------------+------------------+-----------------+--------+-----------------+------------+------------+------------+-----------------+--------------+-----------+---------------------+----------------+------------------+---------------------+----------------+-----------------+---------------+---------------+------------------+--------------+-------------+--------------+-------------+---------+----------------+-----------------+---------------+--------+\n",
      "|CRASH_ID|UNIT_NBR|PRSN_NBR|PRSN_TYPE_ID|PRSN_OCCPNT_POS_ID|PRSN_INJRY_SEV_ID|PRSN_AGE|PRSN_ETHNICITY_ID|PRSN_GNDR_ID|PRSN_EJCT_ID|PRSN_REST_ID|   PRSN_AIRBAG_ID|PRSN_HELMET_ID|PRSN_SOL_FL|PRSN_ALC_SPEC_TYPE_ID|PRSN_ALC_RSLT_ID|PRSN_BAC_TEST_RSLT|PRSN_DRG_SPEC_TYPE_ID|PRSN_DRG_RSLT_ID|DRVR_DRG_CAT_1_ID|PRSN_DEATH_TIME|INCAP_INJRY_CNT|NONINCAP_INJRY_CNT|POSS_INJRY_CNT|NON_INJRY_CNT|UNKN_INJRY_CNT|TOT_INJRY_CNT|DEATH_CNT|DRVR_LIC_TYPE_ID|DRVR_LIC_STATE_ID|DRVR_LIC_CLS_ID|DRVR_ZIP|\n",
      "+--------+--------+--------+------------+------------------+-----------------+--------+-----------------+------------+------------+------------+-----------------+--------------+-----------+---------------------+----------------+------------------+---------------------+----------------+-----------------+---------------+---------------+------------------+--------------+-------------+--------------+-------------+---------+----------------+-----------------+---------------+--------+\n",
      "|14768622|       1|       1|      DRIVER|        FRONT LEFT|      NOT INJURED|      27|         HISPANIC|        MALE|          NO|        NONE|DEPLOYED MULTIPLE|NOT APPLICABLE|          N|                BLOOD|        Positive|             0.225|                 NONE|  NOT APPLICABLE|   NOT APPLICABLE|           NULL|              0|                 0|             0|            1|             0|            0|        0|  DRIVER LICENSE|            Texas|        CLASS C|   77357|\n",
      "+--------+--------+--------+------------+------------------+-----------------+--------+-----------------+------------+------------+------------+-----------------+--------------+-----------+---------------------+----------------+------------------+---------------------+----------------+-----------------+---------------+---------------+------------------+--------------+-------------+--------------+-------------+---------+----------------+-----------------+---------------+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "\n",
      "\n",
      "Reading dataset: restrict_use:\n",
      "+--------+--------+-------------------+\n",
      "|CRASH_ID|UNIT_NBR|DRVR_LIC_RESTRIC_ID|\n",
      "+--------+--------+-------------------+\n",
      "|14768622|       1|               NONE|\n",
      "+--------+--------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "\n",
      "\n",
      "Reading dataset: units_use:\n",
      "+--------+--------+-------------+-------------+----------+----------------+-----------------+------------+------------+-----------+----------+--------------------+---------------+--------+-----------------+----------------+--------------------+-----------------+--------------+--------------------+-----------------+--------------+------------------+---------------+--------------------+--------------------+------------------+-------------------+---------------+---------------------+---------------+------------------+--------------+-------------+--------------+-------------+---------+\n",
      "|CRASH_ID|UNIT_NBR| UNIT_DESC_ID|VEH_PARKED_FL|VEH_HNR_FL|VEH_LIC_STATE_ID|              VIN|VEH_MOD_YEAR|VEH_COLOR_ID|VEH_MAKE_ID|VEH_MOD_ID|    VEH_BODY_STYL_ID|EMER_RESPNDR_FL|OWNR_ZIP|FIN_RESP_PROOF_ID|FIN_RESP_TYPE_ID|  VEH_DMAG_AREA_1_ID|VEH_DMAG_SCL_1_ID|FORCE_DIR_1_ID|  VEH_DMAG_AREA_2_ID|VEH_DMAG_SCL_2_ID|FORCE_DIR_2_ID|VEH_INVENTORIED_FL|VEH_TRANSP_NAME|     VEH_TRANSP_DEST|  CONTRIB_FACTR_1_ID|CONTRIB_FACTR_2_ID|CONTRIB_FACTR_P1_ID|VEH_TRVL_DIR_ID|FIRST_HARM_EVT_INV_ID|INCAP_INJRY_CNT|NONINCAP_INJRY_CNT|POSS_INJRY_CNT|NON_INJRY_CNT|UNKN_INJRY_CNT|TOT_INJRY_CNT|DEATH_CNT|\n",
      "+--------+--------+-------------+-------------+----------+----------------+-----------------+------------+------------+-----------+----------+--------------------+---------------+--------+-----------------+----------------+--------------------+-----------------+--------------+--------------------+-----------------+--------------+------------------+---------------+--------------------+--------------------+------------------+-------------------+---------------+---------------------+---------------+------------------+--------------+-------------+--------------+-------------+---------+\n",
      "|14768622|       1|MOTOR VEHICLE|            N|         N|              TX|4S2CK57D1X4381118|        1999|         GRY|      ISUZU|     AMIGO|PASSENGER CAR, 4-...|              Y|   77357|                2|              NA|FRONT END DAMAGE ...|        DAMAGED 3|             1|LEFT SIDE AND TOP...|        DAMAGED 4|             3|                 Y|VALENTIN TOWING|20288 FM 1314 POR...|UNDER INFLUENCE -...|                NA|                 NA|           EAST|                    Y|              0|                 0|             0|            1|             0|            0|        0|\n",
      "+--------+--------+-------------+-------------+----------+----------------+-----------------+------------+------------+-----------+----------+--------------------+---------------+--------+-----------------+----------------+--------------------+-----------------+--------------+--------------------+-----------------+--------------+------------------+---------------+--------------------+--------------------+------------------+-------------------+---------------+---------------------+---------------+------------------+--------------+-------------+--------------+-------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_charges,df_damages,df_endorse_use,df_primary_person_use,df_restrict_use,df_units_use = read_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize sparkSession and read config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytics 1: Find the number of crashes (accidents) in which number of males killed are greater than 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of crashes in which males killed are greater than 2 : 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "class CrashCount:\n",
    "    # read_all_data()\n",
    "    def __init__(self, primary_person_df: DataFrame):\n",
    "        self.primary_person_df = primary_person_df\n",
    "\n",
    "    def run_analysis(self, gender: str = 'MALE', threshold: int = 2) -> int:\n",
    "        \"\"\"\n",
    "        Run the analysis to count crashes where the number of deaths per crash \n",
    "        for a specific gender exceeds a threshold.\n",
    "\n",
    "        Args:\n",
    "            gender (str): The gender to filter (default: 'MALE').\n",
    "            threshold (int): The minimum number of deaths per crash to count (default: 2).\n",
    "\n",
    "        Returns:\n",
    "            int: The count of crashes meeting the criteria.\n",
    "        \"\"\"\n",
    "        # Filter DataFrame for the specified gender and death count\n",
    "        filtered_df = self.primary_person_df.filter(\n",
    "            (self.primary_person_df[\"PRSN_GNDR_ID\"] == gender) &\n",
    "            (self.primary_person_df[\"DEATH_CNT\"] == 1)            # filtering if death occured\n",
    "        )\n",
    "        \n",
    "        # Count the number of deaths per crash\n",
    "        count_df = filtered_df.groupBy(\"CRASH_ID\").agg(count(\"*\").alias(\"death_per_crash\"))\n",
    "        \n",
    "        # Filter crashes where death count exceeds the threshold\n",
    "        filtered_count_df = count_df.filter(count_df[\"death_per_crash\"] > threshold)\n",
    "        \n",
    "        # Return the count of such crashes\n",
    "        return filtered_count_df.count()\n",
    "\n",
    "\n",
    "solution1 = CrashCount(df_primary_person_use)\n",
    "\n",
    "print(\"Number of crashes in which males killed are greater than 2 :\", solution1.run_analysis('MALE', 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tAnalysis 2: How many two wheelers are booked for crashes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of two wheelers booked for crashes : 948\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr, lower\n",
    "\n",
    "class CountTwoWheelers:\n",
    "    def __init__(self, df_units_use):\n",
    "        self.df_units_use = df_units_use\n",
    "        \n",
    "    def two_wheelers(self):\n",
    "        return self.df_units_use.filter(\n",
    "            (instr(lower(self.df_units_use.VEH_BODY_STYL_ID), \"motorcycle\") > 0) |\n",
    "            (self.df_units_use.UNIT_DESC_ID == \"PEDALCYCLIST\")\n",
    "        ).count()\n",
    "\n",
    "counttwowheelers  = CountTwoWheelers(df_units_use)\n",
    "print(\"No. of two wheelers booked for crashes :\", counttwowheelers.two_wheelers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tAnalysis 3: Determine the Top 5 Vehicle Makes of the cars present in the crashes in which driver died and Airbags did not deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------+\n",
      "|  VEH_MAKE_ID|Death_per_vehicle_make_without_airbags|\n",
      "+-------------+--------------------------------------+\n",
      "|       NISSAN|                                     4|\n",
      "|    CHEVROLET|                                     3|\n",
      "|         FORD|                                     2|\n",
      "|        HONDA|                                     2|\n",
      "|     CADILLAC|                                     1|\n",
      "|      PONTIAC|                                     1|\n",
      "|MERCEDES-BENZ|                                     1|\n",
      "|        BUICK|                                     1|\n",
      "|          KIA|                                     1|\n",
      "|     CHRYSLER|                                     1|\n",
      "+-------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import instr, lower, count, dense_rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class CarMakeEvaluation:\n",
    "    def __init__(self, primary_person_df: DataFrame, units_df: DataFrame):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "\n",
    "    def filter_and_aggregate(self, body_style_keyword: str, person_type: str, death_count: int, airbag_status: str, rank_threshold: int = 5) -> DataFrame:\n",
    "        window = Window.orderBy(col(\"Death_per_vehicle_make_without_airbags\").desc())\n",
    "        \n",
    "        return (\n",
    "            self.primary_person_df\n",
    "            .join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                (instr(lower(self.units_df.VEH_BODY_STYL_ID), body_style_keyword) > 0) &\n",
    "                (self.primary_person_df.PRSN_TYPE_ID == person_type) &\n",
    "                (self.primary_person_df.DEATH_CNT == death_count) &\n",
    "                (self.primary_person_df.PRSN_AIRBAG_ID == airbag_status)\n",
    "            )\n",
    "            .groupBy(self.units_df.VEH_MAKE_ID)\n",
    "            .agg(count(\"*\").alias(\"Death_per_vehicle_make_without_airbags\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") < rank_threshold + 1)\n",
    "            .drop(\"rank\")\n",
    "        )\n",
    "    \n",
    "carmakeevaluation = CarMakeEvaluation(df_primary_person_use, df_units_use)\n",
    "carmakeevaluation.filter_and_aggregate(body_style_keyword ='car', \n",
    "    person_type=\"DRIVER\", \n",
    "    death_count=1, \n",
    "    airbag_status=\"NOT DEPLOYED\",\n",
    "    rank_threshold=5\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tAnalysis 4: Determine number of Vehicles with driver having valid licences involved in hit and run? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2450"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class DriverHitAndRunAnalysis:\n",
    "    def __init__(self, primary_person_df: DataFrame, units_df: DataFrame):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "\n",
    "    def filter_and_count(self, person_type: str, veh_honor_flag: str, valid_lic_types: list, invalid_lic_classes: list) -> int:\n",
    "        return (\n",
    "            self.primary_person_df\n",
    "            .join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                (self.primary_person_df.PRSN_TYPE_ID == person_type) &\n",
    "                (self.units_df.VEH_HNR_FL == veh_honor_flag) &\n",
    "                (\n",
    "                    self.primary_person_df.DRVR_LIC_TYPE_ID.isin(valid_lic_types) &\n",
    "                    (~self.primary_person_df.DRVR_LIC_CLS_ID.isin(invalid_lic_classes))\n",
    "                )\n",
    "            )\n",
    "            .select(\"CRASH_ID\", \"UNIT_NBR\")\n",
    "            .distinct()\n",
    "            .count()\n",
    "        )\n",
    "\n",
    "\n",
    "driverhitandrun = DriverHitAndRunAnalysis(df_primary_person_use, df_units_use)\n",
    "driverhitandrun.filter_and_count(\n",
    "    person_type=\"DRIVER\",\n",
    "    veh_honor_flag=\"Y\",\n",
    "    valid_lic_types=[\"DRIVER LICENSE\", \"COMMERCIAL DRIVER LIC\", \"OCCUPATIONAL\"],\n",
    "    invalid_lic_classes=[\"UNLICENSED\", \"NA\", \"UNKNOWN\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tAnalysis 5: Which state has highest number of accidents in which females are not involved? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------------------+\n",
      "|DRVR_LIC_STATE_ID|Non Women Driver Accident Cases|\n",
      "+-----------------+-------------------------------+\n",
      "|            Texas|                          61022|\n",
      "+-----------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import countDistinct, dense_rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class StateWithHighestAccidents:\n",
    "    def __init__(self, primary_person_df: DataFrame):\n",
    "        self.primary_person_df = primary_person_df\n",
    "\n",
    "    def filter_and_aggregate(self, gender_exclusion: str, person_type: str, group_by_column: str, rank_column: str, rank_value: int = 1) -> DataFrame:\n",
    "        window = Window.orderBy(col(rank_column).desc())\n",
    "        \n",
    "        return (\n",
    "            self.primary_person_df\n",
    "            .filter((self.primary_person_df.PRSN_GNDR_ID != gender_exclusion) &\n",
    "                    (self.primary_person_df.PRSN_TYPE_ID == person_type))\n",
    "            .groupBy(group_by_column)\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(rank_column))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") == rank_value)\n",
    "            .drop(\"rank\")\n",
    "        )\n",
    "    \n",
    "statewithhighestacc = StateWithHighestAccidents(df_primary_person_use)\n",
    "df_states_having_highest_accidents_without_women = statewithhighestacc.filter_and_aggregate(\n",
    "    gender_exclusion=\"FEMALE\",\n",
    "    person_type=\"DRIVER\",\n",
    "    group_by_column=\"DRVR_LIC_STATE_ID\",\n",
    "    rank_column=\"Non Women Driver Accident Cases\",\n",
    "    rank_value=1)\n",
    "\n",
    "df_states_having_highest_accidents_without_women.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tAnalysis 6: Which are the Top 3rd to 5th VEH_MAKE_IDs that contribute to a largest number of injuries including death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------+----+\n",
      "|VEH_MAKE_ID|total_injuries_sum_per_make|rank|\n",
      "+-----------+---------------------------+----+\n",
      "|       FORD|                       12.0|   3|\n",
      "|      DODGE|                       10.0|   4|\n",
      "|     TOYOTA|                        9.0|   5|\n",
      "+-----------+---------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class TopVehicleCategories:\n",
    "    def __init__(self, df_units_use):\n",
    "        self.df_units_use = df_units_use\n",
    "\n",
    "    def get_top_reqd_categories(self, lower_range, upper_range):\n",
    "        # Filter rows where DEATH_CNT is not 0 and VEH_MAKE_ID is not 'NA'\n",
    "        df_filtered = self.df_units_use.filter(\n",
    "            (col(\"DEATH_CNT\") != 0) & (col(\"VEH_MAKE_ID\") != 'NA')\n",
    "        )\n",
    "        \n",
    "        # Aggregate data and calculate total injuries per vehicle make\n",
    "        df_aggregated = df_filtered.groupBy(\"VEH_MAKE_ID\").agg(\n",
    "            sum(\"TOT_INJRY_CNT\").alias(\"total_injuries_sum_per_make\")\n",
    "        )\n",
    "        \n",
    "        # Create a ranking column based on total injuries\n",
    "        window_spec = Window.orderBy(col(\"total_injuries_sum_per_make\").desc())\n",
    "        df_ranked = df_aggregated.withColumn(\n",
    "            \"rank\", dense_rank().over(window_spec)\n",
    "        )\n",
    "        \n",
    "        # Filter based on the rank range\n",
    "        df_top_cat = df_ranked.filter(\n",
    "            (col(\"rank\") >= lower_range) & (col(\"rank\") <= upper_range)\n",
    "        )\n",
    "        \n",
    "        return df_top_cat\n",
    "\n",
    "# Call\n",
    "topcategories = TopVehicleCategories(df_units_use)\n",
    "topcategories.get_top_reqd_categories(3, 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tAnalysis 7: For all the body styles involved in crashes, mention the top ethnic user group of each unique body style  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|VEH_BODY_STYL_ID|PRSN_ETHNICITY_ID|\n",
      "+----------------+-----------------+\n",
      "|       AMBULANCE|            WHITE|\n",
      "|             BUS|            BLACK|\n",
      "|  FARM EQUIPMENT|            WHITE|\n",
      "|      FIRE TRUCK|            WHITE|\n",
      "|      MOTORCYCLE|            WHITE|\n",
      "+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class CrashesPerEthnic:\n",
    "    def __init__(self, primary_person_df, units_df):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "        \n",
    "    def count_crashes(self, rank):\n",
    "        \n",
    "        df_count_of_crashes = (\n",
    "            self.primary_person_df.join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                (~self.units_df.VEH_BODY_STYL_ID.isin([\"NA\", \"UNKNOWN\", \"NOT REPORTED\", \"OTHER  (EXPLAIN IN NARRATIVE)\"])) &\n",
    "                (~self.primary_person_df.PRSN_ETHNICITY_ID.isin([\"NA\", \"UNKNOWN\"]))\n",
    "            )\n",
    "            .groupBy(\"VEH_BODY_STYL_ID\", \"PRSN_ETHNICITY_ID\")\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_crashes\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(\n",
    "                Window.partitionBy(\"VEH_BODY_STYL_ID\").orderBy(col(\"Count_of_crashes\").desc())\n",
    "                )\n",
    "            )\n",
    "            .filter(col(\"rank\") == rank)\n",
    "            .drop(\"rank\", \"Count_of_crashes\")\n",
    "        )\n",
    "        \n",
    "        return df_count_of_crashes\n",
    "    \n",
    "crashperethnic = CrashesPerEthnic(df_primary_person_use,df_units_use)\n",
    "df_count_of_crashes = crashperethnic.count_crashes(1)\n",
    "df_count_of_crashes.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tAnalysis 8: Among the crashed cars, what are the Top 5 Zip Codes with highest number crashes with alcohols as the contributing factor to a crash (Use Driver Zip Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----+\n",
      "|DRVR_ZIP|Count_of_crashes|rank|\n",
      "+--------+----------------+----+\n",
      "|   75052|              27|   1|\n",
      "|   75067|              26|   2|\n",
      "|   76010|              26|   2|\n",
      "|   78521|              24|   3|\n",
      "|   78130|              21|   4|\n",
      "|   78550|              20|   5|\n",
      "|   78745|              20|   5|\n",
      "+--------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, instr, lower, col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class DrunkAndDriveCases:\n",
    "    def __init__(self, primary_person_df, units_df, zip_code_col=\"DRVR_ZIP\",value = 5):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "        self.zip_code_col = zip_code_col\n",
    "        self.value = value\n",
    "\n",
    "    def calculate_drunk_and_drive_cases(self):\n",
    "        \n",
    "        alcohol_condition = (\n",
    "            (instr(lower(self.units_df.CONTRIB_FACTR_1_ID), \"alcohol\") > 0) |\n",
    "            (instr(lower(self.units_df.CONTRIB_FACTR_P1_ID), \"alcohol\") > 0) |\n",
    "            (instr(lower(self.units_df.CONTRIB_FACTR_2_ID), \"alcohol\") > 0)\n",
    "        )\n",
    "        \n",
    "        df_drunk_and_drive_cases = (\n",
    "            self.primary_person_df.join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                alcohol_condition &\n",
    "                (self.primary_person_df[self.zip_code_col] != 'NULL') &\n",
    "                (instr(lower(self.units_df.VEH_BODY_STYL_ID), 'car') > 0)\n",
    "            )\n",
    "            .groupBy(self.zip_code_col)\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_crashes\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(\n",
    "                Window.orderBy(col(\"Count_of_crashes\").desc())\n",
    "            ))\n",
    "            .filter(col(\"rank\") <= self.value)\n",
    "            # .drop(\"rank\")\n",
    "        )\n",
    "        \n",
    "        return df_drunk_and_drive_cases\n",
    "    \n",
    "drunk_drive_cases = DrunkAndDriveCases(df_primary_person_use, df_units_use, zip_code_col=\"DRVR_ZIP\",value = 5)\n",
    "drunk_drive_cases.calculate_drunk_and_drive_cases().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tAnalysis 9: Count of Distinct Crash IDs where No Damaged Property was observed and Damage Level (VEH_DMAG_SCL~) is above 4 and car avails Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|Count_of_distinct_crashIds|\n",
      "+--------------------------+\n",
      "|                         8|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "class CrashIdNoDamageProperty:\n",
    "    def __init__(self, units_df, damages_df):\n",
    "        self.units_df = units_df\n",
    "        self.damages_df = damages_df\n",
    "        \n",
    "    def calculate_crashes(self):\n",
    "        df_no_damage_property = (\n",
    "            self.units_df.join(self.damages_df, \"CRASH_ID\", \"inner\")\n",
    "            .filter(\n",
    "                (\n",
    "                    ((self.units_df.VEH_DMAG_SCL_1_ID).isin(\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\")) |\n",
    "                    ((self.units_df.VEH_DMAG_SCL_2_ID).isin(\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\"))\n",
    "                ) &\n",
    "                (self.damages_df.DAMAGED_PROPERTY == \"NONE\") &\n",
    "                (self.units_df.FIN_RESP_TYPE_ID.isin(\"LIABILITY INSURANCE POLICY\", \"PROOF OF LIABILITY INSURANCE\"))\n",
    "            )\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_distinct_crashIds\"))\n",
    "        )\n",
    "        \n",
    "        return df_no_damage_property\n",
    "    \n",
    "crash_id_no_damage_property = CrashIdNoDamageProperty(df_units_use, df_damages)\n",
    "crash_id_no_damage_property.calculate_crashes().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tAnalysis 10: Determine the Top 5 Vehicle Makes where drivers are charged with speeding related offences, has licensed Drivers, used top 10 used vehicle colours and has car licensed with the Top 25 states with highest number of offences (to be deduced from the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 vehicle makers with speeding related offenses:\n",
      "+-----------+\n",
      "|VEH_MAKE_ID|\n",
      "+-----------+\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, col, dense_rank, instr, lower\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class ReqdTop5VehicleMakes:\n",
    "    def __init__(self, primary_person_df, units_df, charges_df):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "        self.charges_df = charges_df\n",
    "    \n",
    "    def top25_states(self, limit=25):\n",
    "\n",
    "        top_states_df = (\n",
    "            self.primary_person_df.filter(~self.primary_person_df.DRVR_LIC_STATE_ID.isin(\"Unknown\", \"NA\", \"Other\"))\n",
    "            .join(self.charges_df, [\"CRASH_ID\", \"PRSN_NBR\", \"UNIT_NBR\"], \"inner\")\n",
    "            .groupBy(\"DRVR_LIC_STATE_ID\")\n",
    "            .agg(countDistinct(self.charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(\n",
    "                Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "            ))\n",
    "            .filter(col(\"rank\") <= limit)\n",
    "            .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    "        )\n",
    "        return [row.DRVR_LIC_STATE_ID for row in top_states_df.collect()]\n",
    "\n",
    "    \n",
    "    def top10_colors(self, limit=10):\n",
    "        window = Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "        top_colors_df = (\n",
    "            self.units_df.filter(~self.units_df.VEH_COLOR_ID.isin(\"NA\"))\n",
    "            .join(self.charges_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .groupBy(\"VEH_COLOR_ID\")\n",
    "            .agg(countDistinct(self.charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") <= limit)\n",
    "            .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    "        )\n",
    "        return [row.VEH_COLOR_ID for row in top_colors_df.collect()]\n",
    "    \n",
    "    def top_vehicle_makers(self, states, colors, limit=5):\n",
    "        window = Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "        top_makers_df = (\n",
    "            self.primary_person_df.join(self.charges_df, [\"CRASH_ID\", \"PRSN_NBR\", \"UNIT_NBR\"], \"inner\")\n",
    "            .join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                (\n",
    "                    (self.primary_person_df.DRVR_LIC_TYPE_ID.isin(\"DRIVER LICENSE\", \"COMMERCIAL DRIVER LIC\", \"OCCUPATIONAL\")) |\n",
    "                    (~self.primary_person_df.DRVR_LIC_CLS_ID.isin(\"UNLICENSED\", \"NA\", \"UNKNOWN\"))\n",
    "                ) &\n",
    "                (instr(lower(self.charges_df.CHARGE), \"speed\") > 0) &\n",
    "                (self.units_df.VEH_COLOR_ID.isin(colors)) &\n",
    "                (self.units_df.VEH_LIC_STATE_ID.isin(states))\n",
    "            )\n",
    "            .groupBy(\"VEH_MAKE_ID\")\n",
    "            .agg(countDistinct(self.charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") <= limit)\n",
    "            .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    "        )\n",
    "        return top_makers_df\n",
    "    \n",
    "vehicle_makes = ReqdTop5VehicleMakes(df_primary_person_use, df_units_use, df_charges)\n",
    "top25_states = vehicle_makes.top25_states()\n",
    "top10_colors = vehicle_makes.top10_colors()\n",
    "\n",
    "top_makers = vehicle_makes.top_vehicle_makers(top25_states, top10_colors)\n",
    "\n",
    "print(\"Top 5 vehicle makers with speeding related offenses:\")\n",
    "top_makers.show()   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
